{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pickle.load(open(\"New Train Embeddings/train_labels.pkl\", \"rb\"))\n",
    "labels = labels[:156]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_embeds = pickle.load(open(\"New Train Embeddings/nmt_embeddings.pkl\", \"rb\"))\n",
    "adapter_embeds = pickle.load(open(\"New Train Embeddings/adapter_embeddings.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_embeds_concat = torch.cat(nmt_embeds, dim=0)\n",
    "adapter_embeds_concat = torch.cat(adapter_embeds, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_concat = torch.cat(labels, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4992, 4992, 4992)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_concat), len(nmt_embeds_concat), len(adapter_embeds_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, adapter_embeds, nmt_embeds, labels):\n",
    "        self.adapter_embeds = adapter_embeds\n",
    "        self.nmt_embeds = nmt_embeds\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.adapter_embeds[idx], self.nmt_embeds[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(adapter_embeds_concat, nmt_embeds_concat, labels_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [int(len(dataset) * 0.8), len(dataset) - int(len(dataset) * 0.8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Co-Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoAttentionModel(nn.Module):\n",
    "    def __init__(self, embed_dim, num_labels):\n",
    "        super(CoAttentionModel, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self.W_b = nn.Parameter(torch.Tensor(embed_dim, embed_dim))\n",
    "        nn.init.xavier_uniform_(self.W_b)\n",
    "\n",
    "        self.transform1 = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        self.transform2 = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, num_labels)\n",
    "        )\n",
    "\n",
    "        self.batch_norm = nn.BatchNorm1d(embed_dim)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "\n",
    "        x1_transformed = self.transform1(x1)\n",
    "        x2_transformed = self.transform2(x2)\n",
    "\n",
    "        affinity = torch.matmul(x1_transformed, self.W_b)\n",
    "        affinity = torch.matmul(affinity, x2_transformed.transpose(1, 2))\n",
    "\n",
    "        attention_weights1 = F.softmax(affinity, dim=2)\n",
    "        attention_weights2 = F.softmax(affinity.transpose(1, 2), dim=2)\n",
    "\n",
    "        attended_features1 = torch.matmul(attention_weights1, x2_transformed)\n",
    "        attended_features2 = torch.matmul(attention_weights2, x1_transformed)\n",
    "        \n",
    "        attended_features = attended_features1 + attended_features2\n",
    "        attended_features = self.batch_norm(attended_features.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        attended_features = attended_features.mean(dim=1)\n",
    "\n",
    "        logits = self.classifier(attended_features)\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 1:   0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 1: 100%|██████████| 125/125 [00:31<00:00,  3.91it/s]\n",
      "Validation 1: 100%|██████████| 32/32 [00:02<00:00, 13.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train Loss: 0.2967, Val Loss: 0.1788\n",
      "Train Accuracy: 0.2489, Val Accuracy: 0.3213\n",
      "Train Macro F1: 0.3912, Val Macro F1: 0.4730\n",
      "Train Weighted F1: 0.6623, Val Weighted F1: 0.7308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 2: 100%|██████████| 125/125 [00:32<00:00,  3.83it/s]\n",
      "Validation 2: 100%|██████████| 32/32 [00:02<00:00, 12.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "Train Loss: 0.1729, Val Loss: 0.1604\n",
      "Train Accuracy: 0.3321, Val Accuracy: 0.3273\n",
      "Train Macro F1: 0.4985, Val Macro F1: 0.5247\n",
      "Train Weighted F1: 0.7443, Val Weighted F1: 0.7598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 3: 100%|██████████| 125/125 [00:32<00:00,  3.79it/s]\n",
      "Validation 3: 100%|██████████| 32/32 [00:02<00:00, 11.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "Train Loss: 0.1589, Val Loss: 0.1556\n",
      "Train Accuracy: 0.3514, Val Accuracy: 0.3463\n",
      "Train Macro F1: 0.5522, Val Macro F1: 0.5640\n",
      "Train Weighted F1: 0.7730, Val Weighted F1: 0.7728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 4: 100%|██████████| 125/125 [00:33<00:00,  3.71it/s]\n",
      "Validation 4: 100%|██████████| 32/32 [00:02<00:00, 10.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "Train Loss: 0.1519, Val Loss: 0.1522\n",
      "Train Accuracy: 0.3641, Val Accuracy: 0.3443\n",
      "Train Macro F1: 0.5840, Val Macro F1: 0.5727\n",
      "Train Weighted F1: 0.7865, Val Weighted F1: 0.7767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 5: 100%|██████████| 125/125 [00:35<00:00,  3.54it/s]\n",
      "Validation 5: 100%|██████████| 32/32 [00:02<00:00, 10.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "Train Loss: 0.1464, Val Loss: 0.1505\n",
      "Train Accuracy: 0.3694, Val Accuracy: 0.3544\n",
      "Train Macro F1: 0.5940, Val Macro F1: 0.5953\n",
      "Train Weighted F1: 0.7941, Val Weighted F1: 0.7815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 6: 100%|██████████| 125/125 [00:33<00:00,  3.69it/s]\n",
      "Validation 6: 100%|██████████| 32/32 [00:02<00:00, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:\n",
      "Train Loss: 0.1419, Val Loss: 0.1531\n",
      "Train Accuracy: 0.3862, Val Accuracy: 0.3594\n",
      "Train Macro F1: 0.6258, Val Macro F1: 0.6163\n",
      "Train Weighted F1: 0.8042, Val Weighted F1: 0.7876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 7: 100%|██████████| 125/125 [00:32<00:00,  3.90it/s]\n",
      "Validation 7: 100%|██████████| 32/32 [00:02<00:00, 12.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:\n",
      "Train Loss: 0.1364, Val Loss: 0.1491\n",
      "Train Accuracy: 0.3967, Val Accuracy: 0.3614\n",
      "Train Macro F1: 0.6381, Val Macro F1: 0.6194\n",
      "Train Weighted F1: 0.8122, Val Weighted F1: 0.7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 8: 100%|██████████| 125/125 [00:31<00:00,  3.94it/s]\n",
      "Validation 8: 100%|██████████| 32/32 [00:02<00:00, 12.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:\n",
      "Train Loss: 0.1328, Val Loss: 0.1542\n",
      "Train Accuracy: 0.4017, Val Accuracy: 0.3393\n",
      "Train Macro F1: 0.6490, Val Macro F1: 0.6247\n",
      "Train Weighted F1: 0.8163, Val Weighted F1: 0.7883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 9: 100%|██████████| 125/125 [00:31<00:00,  3.94it/s]\n",
      "Validation 9: 100%|██████████| 32/32 [00:02<00:00, 12.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:\n",
      "Train Loss: 0.1294, Val Loss: 0.1472\n",
      "Train Accuracy: 0.4110, Val Accuracy: 0.3333\n",
      "Train Macro F1: 0.6570, Val Macro F1: 0.6411\n",
      "Train Weighted F1: 0.8232, Val Weighted F1: 0.7893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 10: 100%|██████████| 125/125 [00:32<00:00,  3.90it/s]\n",
      "Validation 10: 100%|██████████| 32/32 [00:02<00:00, 12.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:\n",
      "Train Loss: 0.1253, Val Loss: 0.1510\n",
      "Train Accuracy: 0.4145, Val Accuracy: 0.3413\n",
      "Train Macro F1: 0.6781, Val Macro F1: 0.6165\n",
      "Train Weighted F1: 0.8298, Val Weighted F1: 0.7864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "model = CoAttentionModel(embed_dim=768, num_labels=21)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_macro_f1s = []\n",
    "val_macro_f1s = []\n",
    "train_weighted_f1s = []\n",
    "val_weighted_f1s = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    for adapter_embeds, nmt_embeds, labels in tqdm(train_dataloader, desc=f'Training {epoch + 1}'):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(adapter_embeds, nmt_embeds)\n",
    "        loss = loss_function(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_preds.extend(torch.sigmoid(logits).detach().numpy())\n",
    "        train_labels.extend(labels.detach().numpy())\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_preds = np.array(train_preds) > 0.5\n",
    "    train_labels = np.array(train_labels)\n",
    "    train_macro_f1 = f1_score(train_labels, train_preds, average='macro')\n",
    "    train_weighted_f1 = f1_score(train_labels, train_preds, average='weighted')\n",
    "    train_accuracy = accuracy_score(train_labels, train_preds)\n",
    "    train_macro_f1s.append(train_macro_f1)\n",
    "    train_weighted_f1s.append(train_weighted_f1)\n",
    "    train_accs.append(train_accuracy)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    for adapter_embeds, nmt_embeds, labels in tqdm(val_dataloader, desc=f'Validation {epoch + 1}'):\n",
    "        with torch.no_grad():\n",
    "            logits = model(adapter_embeds, nmt_embeds)\n",
    "            loss = loss_function(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "            val_preds.extend(torch.sigmoid(logits).detach().numpy())\n",
    "            val_labels.extend(labels.detach().numpy())\n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_losses.append(val_loss)\n",
    "    val_preds = np.array(val_preds) > 0.5\n",
    "    val_labels = np.array(val_labels)\n",
    "    val_macro_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "    val_weighted_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "    val_macro_f1s.append(val_macro_f1)\n",
    "    val_weighted_f1s.append(val_weighted_f1)\n",
    "    val_accs.append(val_accuracy)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Train Macro F1: {train_macro_f1:.4f}, Val Macro F1: {val_macro_f1:.4f}\")\n",
    "    print(f\"Train Weighted F1: {train_weighted_f1:.4f}, Val Weighted F1: {val_weighted_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"Co Attention Model New/CoAttentionModel.pt\")\n",
    "pickle.dump({\n",
    "    \"train_losses\": train_losses,\n",
    "    \"val_losses\": val_losses,\n",
    "    \"train_macro_f1s\": train_macro_f1s,\n",
    "    \"val_macro_f1s\": val_macro_f1s,\n",
    "    \"train_weighted_f1s\": train_weighted_f1s,\n",
    "    \"val_weighted_f1s\": val_weighted_f1s,\n",
    "    \"train_accs\": train_accs,\n",
    "    \"val_accs\": val_accs\n",
    "}, open(\"Co Attention Model New/CoAttentionModelMetrics.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, num_labels):\n",
    "        super(CrossAttentionModel, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self.attention1 = nn.MultiheadAttention(embed_dim, num_heads=8, batch_first=True, dropout=0.1)\n",
    "        self.attention2 = nn.MultiheadAttention(embed_dim, num_heads=8, batch_first=True, dropout=0.1)\n",
    "\n",
    "        self.transform1 = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        self.transform2 = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, num_labels)\n",
    "        )\n",
    "\n",
    "        self.batch_norm = nn.BatchNorm1d(embed_dim)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "            \n",
    "        x1_transformed = self.transform1(x1)\n",
    "        x2_transformed = self.transform2(x2)\n",
    "\n",
    "        attended_features1, _ = self.attention1(x1_transformed, x2_transformed, x2_transformed)\n",
    "        attended_features2, _ = self.attention2(x2_transformed, x1_transformed, x1_transformed)\n",
    "        \n",
    "        attended_features = attended_features1 + attended_features2\n",
    "        attended_features = self.batch_norm(attended_features.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        attended_features = attended_features.mean(dim=1)\n",
    "\n",
    "        logits = self.classifier(attended_features)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 1:   0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 1: 100%|██████████| 125/125 [01:46<00:00,  1.18it/s]\n",
      "Validation 1: 100%|██████████| 32/32 [00:08<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train Loss: 0.2779, Val Loss: 0.1690\n",
      "Train Accuracy: 0.2697, Val Accuracy: 0.3233\n",
      "Train Macro F1: 0.4213, Val Macro F1: 0.5019\n",
      "Train Weighted F1: 0.6871, Val Weighted F1: 0.7431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 2: 100%|██████████| 125/125 [01:44<00:00,  1.20it/s]\n",
      "Validation 2: 100%|██████████| 32/32 [00:08<00:00,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "Train Loss: 0.1684, Val Loss: 0.1617\n",
      "Train Accuracy: 0.3331, Val Accuracy: 0.3504\n",
      "Train Macro F1: 0.5307, Val Macro F1: 0.5439\n",
      "Train Weighted F1: 0.7574, Val Weighted F1: 0.7650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 3: 100%|██████████| 125/125 [01:46<00:00,  1.18it/s]\n",
      "Validation 3: 100%|██████████| 32/32 [00:08<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "Train Loss: 0.1584, Val Loss: 0.1554\n",
      "Train Accuracy: 0.3551, Val Accuracy: 0.3383\n",
      "Train Macro F1: 0.5790, Val Macro F1: 0.5661\n",
      "Train Weighted F1: 0.7802, Val Weighted F1: 0.7736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 4: 100%|██████████| 125/125 [01:46<00:00,  1.17it/s]\n",
      "Validation 4: 100%|██████████| 32/32 [00:08<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "Train Loss: 0.1516, Val Loss: 0.1514\n",
      "Train Accuracy: 0.3669, Val Accuracy: 0.3453\n",
      "Train Macro F1: 0.6023, Val Macro F1: 0.5957\n",
      "Train Weighted F1: 0.7897, Val Weighted F1: 0.7759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 5: 100%|██████████| 125/125 [01:38<00:00,  1.27it/s]\n",
      "Validation 5: 100%|██████████| 32/32 [00:07<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "Train Loss: 0.1465, Val Loss: 0.1491\n",
      "Train Accuracy: 0.3681, Val Accuracy: 0.3514\n",
      "Train Macro F1: 0.6164, Val Macro F1: 0.6270\n",
      "Train Weighted F1: 0.7965, Val Weighted F1: 0.7883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 6: 100%|██████████| 125/125 [01:44<00:00,  1.19it/s]\n",
      "Validation 6: 100%|██████████| 32/32 [00:08<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:\n",
      "Train Loss: 0.1427, Val Loss: 0.1494\n",
      "Train Accuracy: 0.3722, Val Accuracy: 0.3504\n",
      "Train Macro F1: 0.6357, Val Macro F1: 0.6280\n",
      "Train Weighted F1: 0.8024, Val Weighted F1: 0.7917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 7: 100%|██████████| 125/125 [01:44<00:00,  1.20it/s]\n",
      "Validation 7: 100%|██████████| 32/32 [00:08<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:\n",
      "Train Loss: 0.1375, Val Loss: 0.1545\n",
      "Train Accuracy: 0.3807, Val Accuracy: 0.3473\n",
      "Train Macro F1: 0.6488, Val Macro F1: 0.6346\n",
      "Train Weighted F1: 0.8099, Val Weighted F1: 0.7890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 8: 100%|██████████| 125/125 [01:49<00:00,  1.14it/s]\n",
      "Validation 8: 100%|██████████| 32/32 [00:09<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:\n",
      "Train Loss: 0.1322, Val Loss: 0.1510\n",
      "Train Accuracy: 0.4015, Val Accuracy: 0.3504\n",
      "Train Macro F1: 0.6600, Val Macro F1: 0.6554\n",
      "Train Weighted F1: 0.8178, Val Weighted F1: 0.7949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 9: 100%|██████████| 125/125 [01:51<00:00,  1.12it/s]\n",
      "Validation 9: 100%|██████████| 32/32 [00:09<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:\n",
      "Train Loss: 0.1275, Val Loss: 0.1507\n",
      "Train Accuracy: 0.4075, Val Accuracy: 0.3584\n",
      "Train Macro F1: 0.6762, Val Macro F1: 0.6478\n",
      "Train Weighted F1: 0.8268, Val Weighted F1: 0.7942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 10: 100%|██████████| 125/125 [01:53<00:00,  1.10it/s]\n",
      "Validation 10: 100%|██████████| 32/32 [00:09<00:00,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:\n",
      "Train Loss: 0.1218, Val Loss: 0.1522\n",
      "Train Accuracy: 0.4162, Val Accuracy: 0.3443\n",
      "Train Macro F1: 0.6920, Val Macro F1: 0.6544\n",
      "Train Weighted F1: 0.8351, Val Weighted F1: 0.7962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "model = CrossAttentionModel(embed_dim=768, num_labels=21)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_macro_f1s = []\n",
    "val_macro_f1s = []\n",
    "train_weighted_f1s = []\n",
    "val_weighted_f1s = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    for adapter_embeds, nmt_embeds, labels in tqdm(train_dataloader, desc=f'Training {epoch + 1}'):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(adapter_embeds, nmt_embeds)\n",
    "        loss = loss_function(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_preds.extend(torch.sigmoid(logits).detach().numpy())\n",
    "        train_labels.extend(labels.detach().numpy())\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_preds = np.array(train_preds) > 0.5\n",
    "    train_labels = np.array(train_labels)\n",
    "    train_macro_f1 = f1_score(train_labels, train_preds, average='macro')\n",
    "    train_weighted_f1 = f1_score(train_labels, train_preds, average='weighted')\n",
    "    train_accuracy = accuracy_score(train_labels, train_preds)\n",
    "    train_macro_f1s.append(train_macro_f1)\n",
    "    train_weighted_f1s.append(train_weighted_f1)\n",
    "    train_accs.append(train_accuracy)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    for adapter_embeds, nmt_embeds, labels in tqdm(val_dataloader, desc=f'Validation {epoch + 1}'):\n",
    "        with torch.no_grad():\n",
    "            logits = model(adapter_embeds, nmt_embeds)\n",
    "            loss = loss_function(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "            val_preds.extend(torch.sigmoid(logits).detach().numpy())\n",
    "            val_labels.extend(labels.detach().numpy())\n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_losses.append(val_loss)\n",
    "    val_preds = np.array(val_preds) > 0.5\n",
    "    val_labels = np.array(val_labels)\n",
    "    val_macro_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "    val_weighted_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "    val_macro_f1s.append(val_macro_f1)\n",
    "    val_weighted_f1s.append(val_weighted_f1)\n",
    "    val_accs.append(val_accuracy)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Train Macro F1: {train_macro_f1:.4f}, Val Macro F1: {val_macro_f1:.4f}\")\n",
    "    print(f\"Train Weighted F1: {train_weighted_f1:.4f}, Val Weighted F1: {val_weighted_f1:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"Cross Attention Model New/CrossAttentionModel.pt\")\n",
    "pickle.dump({\n",
    "    \"train_losses\": train_losses,\n",
    "    \"val_losses\": val_losses,\n",
    "    \"train_macro_f1s\": train_macro_f1s,\n",
    "    \"val_macro_f1s\": val_macro_f1s,\n",
    "    \"train_weighted_f1s\": train_weighted_f1s,\n",
    "    \"val_weighted_f1s\": val_weighted_f1s,\n",
    "    \"train_accs\": train_accs,\n",
    "    \"val_accs\": val_accs\n",
    "}, open(\"Cross Attention Model New/CrossAttentionModelMetrics.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple(nn.Module):\n",
    "    def __init__(self, embed_dim, num_labels):\n",
    "        super(Simple, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x = x2.mean(dim=1)\n",
    "        logits = self.classifier(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 1:   0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 1: 100%|██████████| 125/125 [00:01<00:00, 95.27it/s]\n",
      "Validation 1: 100%|██████████| 32/32 [00:00<00:00, 152.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train Loss: 0.3340, Val Loss: 0.2167\n",
      "Train Accuracy: 0.1282, Val Accuracy: 0.2112\n",
      "Train Macro F1: 0.2223, Val Macro F1: 0.3172\n",
      "Train Weighted F1: 0.4988, Val Weighted F1: 0.6239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 2: 100%|██████████| 125/125 [00:01<00:00, 100.08it/s]\n",
      "Validation 2: 100%|██████████| 32/32 [00:00<00:00, 174.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "Train Loss: 0.1985, Val Loss: 0.1853\n",
      "Train Accuracy: 0.2454, Val Accuracy: 0.2863\n",
      "Train Macro F1: 0.3910, Val Macro F1: 0.4495\n",
      "Train Weighted F1: 0.6729, Val Weighted F1: 0.7104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 3: 100%|██████████| 125/125 [00:01<00:00, 101.35it/s]\n",
      "Validation 3: 100%|██████████| 32/32 [00:00<00:00, 159.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "Train Loss: 0.1824, Val Loss: 0.1766\n",
      "Train Accuracy: 0.2785, Val Accuracy: 0.2953\n",
      "Train Macro F1: 0.4631, Val Macro F1: 0.4835\n",
      "Train Weighted F1: 0.7179, Val Weighted F1: 0.7218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 4: 100%|██████████| 125/125 [00:01<00:00, 102.44it/s]\n",
      "Validation 4: 100%|██████████| 32/32 [00:00<00:00, 144.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "Train Loss: 0.1754, Val Loss: 0.1726\n",
      "Train Accuracy: 0.3013, Val Accuracy: 0.3183\n",
      "Train Macro F1: 0.4956, Val Macro F1: 0.5122\n",
      "Train Weighted F1: 0.7347, Val Weighted F1: 0.7431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 5: 100%|██████████| 125/125 [00:01<00:00, 101.29it/s]\n",
      "Validation 5: 100%|██████████| 32/32 [00:00<00:00, 165.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "Train Loss: 0.1717, Val Loss: 0.1690\n",
      "Train Accuracy: 0.3183, Val Accuracy: 0.3203\n",
      "Train Macro F1: 0.5172, Val Macro F1: 0.5107\n",
      "Train Weighted F1: 0.7478, Val Weighted F1: 0.7454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 6: 100%|██████████| 125/125 [00:01<00:00, 94.49it/s] \n",
      "Validation 6: 100%|██████████| 32/32 [00:00<00:00, 161.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:\n",
      "Train Loss: 0.1686, Val Loss: 0.1675\n",
      "Train Accuracy: 0.3216, Val Accuracy: 0.3313\n",
      "Train Macro F1: 0.5292, Val Macro F1: 0.5200\n",
      "Train Weighted F1: 0.7527, Val Weighted F1: 0.7480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 7:  45%|████▍     | 56/125 [00:00<00:00, 102.90it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m train_preds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     18\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 19\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madapter_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnmt_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTraining \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msimple_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnmt_embeds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dell\\OneDrive\\IIITD\\Semester 8\\NLP\\NLP Project\\nlp_venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\Dell\\OneDrive\\IIITD\\Semester 8\\NLP\\NLP Project\\nlp_venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Dell\\OneDrive\\IIITD\\Semester 8\\NLP\\NLP Project\\nlp_venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\OneDrive\\IIITD\\Semester 8\\NLP\\NLP Project\\nlp_venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dell\\OneDrive\\IIITD\\Semester 8\\NLP\\NLP Project\\nlp_venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:277\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dell\\OneDrive\\IIITD\\Semester 8\\NLP\\NLP Project\\nlp_venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:144\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransposed\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Dell\\OneDrive\\IIITD\\Semester 8\\NLP\\NLP Project\\nlp_venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:144\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Dell\\OneDrive\\IIITD\\Semester 8\\NLP\\NLP Project\\nlp_venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:121\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\Dell\\OneDrive\\IIITD\\Semester 8\\NLP\\NLP Project\\nlp_venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:174\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    172\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    173\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "simple_model = Simple(embed_dim=768, num_labels=21)\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(simple_model.parameters(), lr=1e-4)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_macro_f1s = []\n",
    "val_macro_f1s = []\n",
    "train_weighted_f1s = []\n",
    "val_weighted_f1s = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    simple_model.train()\n",
    "    train_loss = 0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    for adapter_embeds, nmt_embeds, labels in tqdm(train_dataloader, desc=f'Training {epoch + 1}'):\n",
    "        optimizer.zero_grad()\n",
    "        logits = simple_model(adapter_embeds, nmt_embeds)\n",
    "        loss = loss_function(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_preds.extend(torch.sigmoid(logits).detach().numpy())\n",
    "        train_labels.extend(labels.detach().numpy())\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_preds = np.array(train_preds) > 0.5\n",
    "    train_labels = np.array(train_labels)\n",
    "    train_macro_f1 = f1_score(train_labels, train_preds, average='macro')\n",
    "    train_weighted_f1 = f1_score(train_labels, train_preds, average='weighted')\n",
    "    train_accuracy = accuracy_score(train_labels, train_preds)\n",
    "    train_macro_f1s.append(train_macro_f1)\n",
    "    train_weighted_f1s.append(train_weighted_f1)\n",
    "    train_accs.append(train_accuracy)\n",
    "    \n",
    "    simple_model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    for adapter_embeds, nmt_embeds, labels in tqdm(val_dataloader, desc=f'Validation {epoch + 1}'):\n",
    "        with torch.no_grad():\n",
    "            logits = simple_model(adapter_embeds, nmt_embeds)\n",
    "            loss = loss_function(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "            val_preds.extend(torch.sigmoid(logits).detach().numpy())\n",
    "            val_labels.extend(labels.detach().numpy())\n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_losses.append(val_loss)\n",
    "    val_preds = np.array(val_preds) > 0.5\n",
    "    val_labels = np.array(val_labels)\n",
    "    val_macro_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "    val_weighted_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "    val_macro_f1s.append(val_macro_f1)\n",
    "    val_weighted_f1s.append(val_weighted_f1)\n",
    "    val_accs.append(val_accuracy)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Train Macro F1: {train_macro_f1:.4f}, Val Macro F1: {val_macro_f1:.4f}\")\n",
    "    print(f\"Train Weighted F1: {train_weighted_f1:.4f}, Val Weighted F1: {val_weighted_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
